import re
import pandas as pd
from fuzzywuzzy import fuzz
from datasketch import MinHash, MinHashLSH
from tqdm.notebook import tqdm
import pandas as pd

#################### GENERAL DATA CLEANING FUNCTIONS ####################

def normalize_post(text):
    """
    Convert the text to lowercase and remove URLs, hashtags, mentions, punctuation,
    and extra whitespace.
    """
    text = text.lower()
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    #text = re.sub(r"[@#]\S+", "", text)  # Remove hashtags and mentions
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    return text.strip()


def remove_spammers(df, spammer_list):
    """
    Remove rows where the 'Author ID' is in the provided spammer_list.
    """
    return df[~df["Author ID"].isin(spammer_list)]


# def remove_duplicates(df, similarity_threshold=90):
#     """
#     Remove near-duplicate posts based on fuzzy matching of the normalized text.
#     Assumes that the DataFrame has a 'Normalized' column.
#     Returns a DataFrame with only the highest-engagement post kept from duplicates.
#     """
#     unique_posts = []
#     final_rows = []

#     # Iterate through the DataFrame rows (assumed sorted by engagement descending)
#     for idx, row in df.iterrows():
#         text_norm = row["Normalized"]
#         is_duplicate = False
#         # Compare with already kept unique posts
#         for unique_text in unique_posts:
#             if fuzz.ratio(text_norm, unique_text) >= similarity_threshold:
#                 is_duplicate = True
#                 break
#         if not is_duplicate:
#             unique_posts.append(text_norm)
#             final_rows.append(row)
#     return pd.DataFrame(final_rows)


#################### NEAR-DUPLICATES HANDLING FUNCTIONS ####################


def tokenize(text):
    return set(text.lower().split())


def create_minhash(tokens, num_perm=128):
    mh = MinHash(num_perm=num_perm)
    for token in tokens:
        mh.update(token.encode("utf8"))
    return mh


def find_near_duplicates(
    df, text_column="Normalized Text", num_perm=512, threshold=0.7
):
    """
    Finds near-duplicate text entries in a DataFrame using MinHash and Locality Sensitive Hashing (LSH).

    MinHash is a technique for quickly estimating the Jaccard similarity between two sets (e.g., sets of words).
    Locality Sensitive Hashing (LSH) allows for efficient approximate nearest neighbor search by hashing similar items
    into the same "buckets" with high probability.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame containing text data to be analyzed for near duplicates.

    text_column : str, default="Normalized Text"
        The name of the column in the DataFrame that contains the normalized text (e.g., lowercase, URL-removed tweets).
        This text will be tokenized and used to compute MinHash signatures.

    num_perm : int, default=256
        The number of hash permutations used in the MinHash signature.
        - Higher values (e.g., 256 or 512) improve the accuracy of the estimated Jaccard similarity between sets.
        - Lower values (e.g., 64 or 128) reduce computation and memory usage but are less accurate.

    threshold : float, default=0.8
        The Jaccard similarity threshold (between 0 and 1) for considering two texts as near duplicates.
        - A value of 0.8 means only pairs with 80% or more token overlap (approximate) will be considered duplicates.
        - Lower this value to find looser matches; raise it to be more strict.

    Returns
    -------
    duplicates_df : pandas.DataFrame
        A DataFrame containing pairs of near-duplicate text entries with the following columns:
        - Index_1, Tweet_1: index and text of the first duplicate
        - Index_2, Tweet_2: index and text of the second duplicate
        - Approx_Jaccard: the threshold used for similarity comparison

    Notes
    -----
    - This function assumes that the input text has been preprocessed (e.g., lowercased, punctuation removed).
    - Tokens are generated by simple whitespace splitting. For better results, consider using custom tokenization.
    - The result is symmetric: if (A, B) is a pair, (B, A) will not be repeated.

    Example
    -------
    >>> duplicates = find_near_duplicates(df, text_column="Normalized Texts", num_perm=256, threshold=0.85)
    >>> print(duplicates.head())
    """

    # def tokenize(text):
    #     return set(text.lower().split())

    

    df = df.copy()
    df["tokens"] = df[text_column].apply(tokenize)

    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
    minhash_dict = {}

    print("Building MinHash signatures and inserting into LSH...")
    for i, row in tqdm(df.iterrows(), total=len(df), desc="MinHash & LSH Insertion"):
        mh = create_minhash(row["tokens"], num_perm=num_perm)
        minhash_dict[i] = mh
        lsh.insert(str(i), mh)
    print("LSH index built.")

    print("Finding near-duplicate pairs...")
    similar_pairs = []
    for i in tqdm(df.index, total=len(df), desc="Finding near-duplicate pairs"):
        mh = minhash_dict[i]
        result = lsh.query(mh)
        dup_indices = [int(x) for x in result if int(x) != int(i)]

        for j in dup_indices:
            similar_pairs.append(
                {
                    "Index_1": i,
                    "Tweet_1": df.loc[i, text_column],
                    "Index_2": j,
                    "Tweet_2": df.loc[j, text_column],
                    "Approx_Jaccard": threshold,
                }
            )

    duplicates_df = pd.DataFrame(similar_pairs)
    duplicates_df["Pair"] = duplicates_df.apply(
        lambda row: tuple(sorted([row["Index_1"], row["Index_2"]])), axis=1
    )
    duplicates_df.drop_duplicates("Pair", inplace=True)
    duplicates_df.drop(columns="Pair", inplace=True)

    print("Number of near-duplicate pairs found:", len(duplicates_df))
    return duplicates_df


def remove_duplicates_keep_highest_engagement(
    df, duplicates_df, engagement_column="engagement"
):
    """
    Memory-efficient function to remove near-duplicate entries from a DataFrame,
    keeping only the entry with the highest engagement value from each group.

    Works with large datasets under limited RAM constraints.
    """
    import pandas as pd
    import networkx as nx

    # Process duplicates in smaller chunks to reduce memory usage
    chunk_size = 10000

    # Create a graph to identify connected components (groups of duplicates)
    G = nx.Graph()

    # Add all unique indices as nodes
    all_indices = set(duplicates_df["Index_1"]).union(set(duplicates_df["Index_2"]))
    G.add_nodes_from(all_indices)

    # Add edges between duplicate pairs in chunks
    for i in range(0, len(duplicates_df), chunk_size):
        chunk = duplicates_df.iloc[i : i + chunk_size]
        for _, row in chunk.iterrows():
            G.add_edge(row["Index_1"], row["Index_2"])

    # Find connected components (groups of duplicates)
    duplicate_groups = list(nx.connected_components(G))
    print(f"Found {len(duplicate_groups)} groups of near-duplicates")

    # Process groups in batches to reduce memory usage
    indices_to_remove = []
    batch_size = 1000  # Process this many groups at once

    for i in range(0, len(duplicate_groups), batch_size):
        batch_groups = duplicate_groups[i : i + batch_size]

        for group in batch_groups:
            group_indices = list(group)

            # Instead of loading all group data at once, just extract engagement values
            group_engagements = df.loc[group_indices, engagement_column]

            # Find the index with highest engagement
            max_engagement_idx = group_engagements.idxmax()

            # Add other indices to removal list
            group_indices.remove(max_engagement_idx)
            indices_to_remove.extend(group_indices)

        # Explicitly clear variables to free memory
        del batch_groups
        del group_engagements

    print(f"Removing {len(indices_to_remove)} duplicates")

    # Create a new dataframe with duplicates removed, this is more memory efficient
    # than creating a new dataframe with only the rows to keep
    filtered_df = df.drop(indices_to_remove)

    return filtered_df


def plot_simplified_duplicate_network(
    duplicates_df, df=None, sample_size=None, save_path=None
):
    """
    Visualizes the network of near-duplicate clusters with group coloring and a discrete legend.

    Parameters:
    -----------
    duplicates_df : DataFrame
        Output from find_near_duplicates()
    df : DataFrame, optional
        Original dataset (only needed to resolve indices)
    sample_size : int, optional
        Number of duplicate pairs to sample for faster rendering

    Returns:
    --------
    None â€“ displays the network graph
    """
    import matplotlib.pyplot as plt
    import networkx as nx
    import matplotlib.cm as cm
    import matplotlib.patches as mpatches

    # Sample if needed
    if sample_size and sample_size < len(duplicates_df):
        viz_df = duplicates_df.sample(sample_size, random_state=42)
        print(f"Sampled {sample_size} of {len(duplicates_df)} duplicate pairs")
    else:
        viz_df = duplicates_df

    # Build graph
    G = nx.Graph()
    for _, row in viz_df.iterrows():
        G.add_edge(row["Index_1"], row["Index_2"])

    # Layout
    pos = nx.spring_layout(G, seed=42)

    # Identify components (groups)
    components = list(nx.connected_components(G))
    node_to_group = {node: i for i, comp in enumerate(components) for node in comp}
    group_ids = sorted(set(node_to_group.values()))

    # Assign each group a unique color from tab20
    cmap = cm.get_cmap("tab20", len(group_ids))
    group_colors = {group_id: cmap(group_id) for group_id in group_ids}

    # Prepare plot
    fig, ax = plt.subplots(figsize=(12, 8))

    # Draw nodes per group (for custom legend)
    for group_id in group_ids:
        nodes_in_group = [n for n in G.nodes if node_to_group[n] == group_id]
        nx.draw_networkx_nodes(
            G,
            pos,
            nodelist=nodes_in_group,
            node_color=[group_colors[group_id]] * len(nodes_in_group),
            node_size=300,
            alpha=0.9,
            ax=ax,
            label=f"Group {group_id}",
        )

    # Draw edges and labels
    nx.draw_networkx_edges(G, pos, edge_color="gray", ax=ax, alpha=0.5)
    nx.draw_networkx_labels(G, pos, font_size=8, ax=ax)

    # Add legend
    handles = [
        mpatches.Patch(color=group_colors[g], label=f"Group {g}") for g in group_ids
    ]
    ax.legend(
        handles=handles, title="Group ID", bbox_to_anchor=(1.02, 1), loc="upper left"
    )

    # Add prominent annotation about node labels
    ax.text(
        0.01,
        -0.08,
        "Each node number = index in the dataset",
        transform=ax.transAxes,
        fontsize=12,
        fontweight="bold",
        color="darkslategray",
        ha="left",
    )

    # Restore plot border
    for spine in ax.spines.values():
        spine.set_visible(True)

    ax.set_title("Network of Near-Duplicate Pairs (Grouped)", fontsize=14)
    ax.axis("on")  # Show the border frame
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Plot saved to {save_path}")
    plt.show()
